{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change in Dataset Construction\n",
    "\n",
    "This notebook aims to change the way the dataset is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    os.chdir(PROJECT_PATH)\n",
    "except NameError:\n",
    "    NOTEBOOK_PATH = Path(os.getcwd())\n",
    "    os.chdir(\"..\")\n",
    "    PROJECT_PATH = Path(os.getcwd())\n",
    "SRC_PATH = PROJECT_PATH / \"src\"\n",
    "DATA_PATH = PROJECT_PATH / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import random\n",
    "random.seed(10)\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.db import DBInterface\n",
    "from src.utils.common import PositionalEncoding\n",
    "import torch\n",
    "from torch.nn import ModuleDict\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\n",
    "    \"ITUB4.SA\",\n",
    "    \"BBDC4.SA\",\n",
    "    \"BBAS3.SA\",\n",
    "    \"SANB11.SA\",\n",
    "    \"BPAC11.SA\",\n",
    "    \"PETR4.SA\"\n",
    "           ]\n",
    "\n",
    "COMP_IND = [\"P/L\", \"PL/ATIVOS\", \"M. EBIT\", \"ROA\", \"CAGR LUCROS 5 ANOS\"]\n",
    "TARGETS_LOW = [col.lower().replace(\".sa\", \"\") for col in TARGETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prep(db, targets):\n",
    "    df = db.read_sql(query=\"SELECT * FROM stocks\")\n",
    "\n",
    "    df_symbol = df[lambda f: f[\"symbol\"].isin(targets)]\n",
    "\n",
    "    assert not df_symbol.empty, \"The dataframe is empty\"\n",
    "\n",
    "    df_pivot = df_symbol.pivot(\n",
    "        index=\"quote_date\",\n",
    "        columns=\"symbol\",\n",
    "        values=[\"close\"],\n",
    "    )\n",
    "    df_pivot.columns = [\n",
    "        stock.lower().replace(\".sa\", \"\") + \"_price\" for _, stock in df_pivot.columns.values\n",
    "    ]    \n",
    "    # focus only on close price for now\n",
    "    df_close = df_pivot.filter(like=\"price\")\n",
    "    \n",
    "    # col_close_to_prices = [col.replace(\"close\", \"price\") for col in df_close.columns]\n",
    "    \n",
    "    # df_price = df_close.copy()\n",
    "    # df_price.columns =col_close_to_prices\n",
    "    \n",
    "    return df_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBInterface()\n",
    "df = df_prep(db, targets = TARGETS).sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fund = pd.read_csv(DATA_PATH / \"company_indicators_top5_banks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/602tgmp162ngdgy2l3rwdpn00000gn/T/ipykernel_56234/392088152.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ind[\"quote_date\"] = pd.to_datetime(df_ind[\"quote_date\"])\n",
      "/var/folders/xk/602tgmp162ngdgy2l3rwdpn00000gn/T/ipykernel_56234/392088152.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ind[\"col\"] = df_ind[\"stock\"] + \"_\" + df_ind[\"indicators\"].str.lower().str.replace(\".\", \"\").str.replace(\" \", \"-\")\n"
     ]
    }
   ],
   "source": [
    "df_ind = df_fund.loc[lambda f: f[\"stock\"].isin(TARGETS_LOW) & f[\"indicators\"].isin(COMP_IND)]\n",
    "df_ind[\"quote_date\"] = pd.to_datetime(df_ind[\"quote_date\"])\n",
    "df_ind[\"col\"] = df_ind[\"stock\"] + \"_\" + df_ind[\"indicators\"].str.lower().str.replace(\".\", \"\").str.replace(\" \", \"-\")\n",
    "df_pivot = df_ind.pivot(\n",
    "    index = \"quote_date\",\n",
    "    columns = \"col\",\n",
    "    values=\"valor\"\n",
    ")\n",
    "df_pivot[\"year\"] = df_pivot.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df.index.year\n",
    "\n",
    "res = df.merge(\n",
    "    df_pivot,\n",
    "    how=\"left\",\n",
    "    on=\"year\",\n",
    ").drop(\"year\", axis=1)\n",
    "\n",
    "res.set_index(df.index, inplace=True)\n",
    "\n",
    "multi = pd.MultiIndex.from_tuples([tuple(col.split(\"_\"))for col in res.columns])\n",
    "\n",
    "res.columns = multi\n",
    "\n",
    "res = res.reorder_levels([0, 1], axis=1).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">bbas3</th>\n",
       "      <th colspan=\"4\" halign=\"left\">bbdc4</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">itub4</th>\n",
       "      <th>petr4</th>\n",
       "      <th colspan=\"6\" halign=\"left\">sanb11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>cagr-lucros-5-anos</th>\n",
       "      <th>m-ebit</th>\n",
       "      <th>p/l</th>\n",
       "      <th>pl/ativos</th>\n",
       "      <th>price</th>\n",
       "      <th>roa</th>\n",
       "      <th>cagr-lucros-5-anos</th>\n",
       "      <th>m-ebit</th>\n",
       "      <th>p/l</th>\n",
       "      <th>pl/ativos</th>\n",
       "      <th>...</th>\n",
       "      <th>pl/ativos</th>\n",
       "      <th>price</th>\n",
       "      <th>roa</th>\n",
       "      <th>price</th>\n",
       "      <th>cagr-lucros-5-anos</th>\n",
       "      <th>m-ebit</th>\n",
       "      <th>p/l</th>\n",
       "      <th>pl/ativos</th>\n",
       "      <th>price</th>\n",
       "      <th>roa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quote_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-10-06</th>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.1616</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>48.849998</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>9.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>33.509998</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.11</td>\n",
       "      <td>27.190001</td>\n",
       "      <td>0.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-05</th>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.1616</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>46.939999</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>9.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>27.639999</td>\n",
       "      <td>0.013</td>\n",
       "      <td>32.730000</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.11</td>\n",
       "      <td>26.680000</td>\n",
       "      <td>0.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-04</th>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.1616</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>46.799999</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>9.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>27.209999</td>\n",
       "      <td>0.013</td>\n",
       "      <td>32.619999</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.11</td>\n",
       "      <td>26.059999</td>\n",
       "      <td>0.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-03</th>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.1616</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>46.450001</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>9.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>33.970001</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.11</td>\n",
       "      <td>25.700001</td>\n",
       "      <td>0.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-02</th>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.1616</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>47.049999</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>9.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>26.940001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>34.119999</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.11</td>\n",
       "      <td>25.920000</td>\n",
       "      <td>0.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.028783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.410173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.052463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.403764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.037993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.408635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.027468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.422993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.078775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.506321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5971 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bbas3                                             \\\n",
       "           cagr-lucros-5-anos  m-ebit   p/l pl/ativos      price     roa   \n",
       "quote_date                                                                 \n",
       "2023-10-06             0.2508  0.1616  4.27      0.08  48.849998  0.0157   \n",
       "2023-10-05             0.2508  0.1616  4.27      0.08  46.939999  0.0157   \n",
       "2023-10-04             0.2508  0.1616  4.27      0.08  46.799999  0.0157   \n",
       "2023-10-03             0.2508  0.1616  4.27      0.08  46.450001  0.0157   \n",
       "2023-10-02             0.2508  0.1616  4.27      0.08  47.049999  0.0157   \n",
       "...                       ...     ...   ...       ...        ...     ...   \n",
       "2000-01-07                NaN     NaN   NaN       NaN   1.028783     NaN   \n",
       "2000-01-06                NaN     NaN   NaN       NaN   1.052463     NaN   \n",
       "2000-01-05                NaN     NaN   NaN       NaN   1.037993     NaN   \n",
       "2000-01-04                NaN     NaN   NaN       NaN   1.027468     NaN   \n",
       "2000-01-03                NaN     NaN   NaN       NaN   1.078775     NaN   \n",
       "\n",
       "                        bbdc4                          ...     itub4  \\\n",
       "           cagr-lucros-5-anos  m-ebit   p/l pl/ativos  ... pl/ativos   \n",
       "quote_date                                             ...             \n",
       "2023-10-06            -0.0096  0.0619  9.46       NaN  ...      0.07   \n",
       "2023-10-05            -0.0096  0.0619  9.46       NaN  ...      0.07   \n",
       "2023-10-04            -0.0096  0.0619  9.46       NaN  ...      0.07   \n",
       "2023-10-03            -0.0096  0.0619  9.46       NaN  ...      0.07   \n",
       "2023-10-02            -0.0096  0.0619  9.46       NaN  ...      0.07   \n",
       "...                       ...     ...   ...       ...  ...       ...   \n",
       "2000-01-07                NaN     NaN   NaN       NaN  ...       NaN   \n",
       "2000-01-06                NaN     NaN   NaN       NaN  ...       NaN   \n",
       "2000-01-05                NaN     NaN   NaN       NaN  ...       NaN   \n",
       "2000-01-04                NaN     NaN   NaN       NaN  ...       NaN   \n",
       "2000-01-03                NaN     NaN   NaN       NaN  ...       NaN   \n",
       "\n",
       "                                  petr4             sanb11                 \\\n",
       "                price    roa      price cagr-lucros-5-anos  m-ebit    p/l   \n",
       "quote_date                                                                  \n",
       "2023-10-06  27.900000  0.013  33.509998             0.0229  0.0732  10.38   \n",
       "2023-10-05  27.639999  0.013  32.730000             0.0229  0.0732  10.38   \n",
       "2023-10-04  27.209999  0.013  32.619999             0.0229  0.0732  10.38   \n",
       "2023-10-03  26.600000  0.013  33.970001             0.0229  0.0732  10.38   \n",
       "2023-10-02  26.940001  0.013  34.119999             0.0229  0.0732  10.38   \n",
       "...               ...    ...        ...                ...     ...    ...   \n",
       "2000-01-07        NaN    NaN   1.410173                NaN     NaN    NaN   \n",
       "2000-01-06        NaN    NaN   1.403764                NaN     NaN    NaN   \n",
       "2000-01-05        NaN    NaN   1.408635                NaN     NaN    NaN   \n",
       "2000-01-04        NaN    NaN   1.422993                NaN     NaN    NaN   \n",
       "2000-01-03        NaN    NaN   1.506321                NaN     NaN    NaN   \n",
       "\n",
       "                                         \n",
       "           pl/ativos      price     roa  \n",
       "quote_date                               \n",
       "2023-10-06      0.11  27.190001  0.0098  \n",
       "2023-10-05      0.11  26.680000  0.0098  \n",
       "2023-10-04      0.11  26.059999  0.0098  \n",
       "2023-10-03      0.11  25.700001  0.0098  \n",
       "2023-10-02      0.11  25.920000  0.0098  \n",
       "...              ...        ...     ...  \n",
       "2000-01-07       NaN        NaN     NaN  \n",
       "2000-01-06       NaN        NaN     NaN  \n",
       "2000-01-05       NaN        NaN     NaN  \n",
       "2000-01-04       NaN        NaN     NaN  \n",
       "2000-01-03       NaN        NaN     NaN  \n",
       "\n",
       "[5971 rows x 31 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>indicators</th>\n",
       "      <th>quote_date</th>\n",
       "      <th>Dolar</th>\n",
       "      <th>IPCA</th>\n",
       "      <th>PIB</th>\n",
       "      <th>Risco-Brasil</th>\n",
       "      <th>Selic Over</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.422</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.951</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8169</th>\n",
       "      <td>1953-05-01</td>\n",
       "      <td>-4.085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>1953-04-01</td>\n",
       "      <td>11.252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8171</th>\n",
       "      <td>1953-03-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>1953-02-01</td>\n",
       "      <td>100.165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173</th>\n",
       "      <td>1953-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8174 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "indicators quote_date    Dolar  IPCA  PIB  Risco-Brasil  Selic Over\n",
       "0          2023-10-06      NaN   NaN  NaN        -1.422         NaN\n",
       "1          2023-10-05      NaN   NaN  NaN         0.957         NaN\n",
       "2          2023-10-04      NaN   NaN  NaN         1.951         NaN\n",
       "3          2023-10-03      NaN   NaN  NaN         2.500         NaN\n",
       "4          2023-10-02      NaN   NaN  NaN        -0.498         NaN\n",
       "...               ...      ...   ...  ...           ...         ...\n",
       "8169       1953-05-01   -4.085   NaN  NaN           NaN         NaN\n",
       "8170       1953-04-01   11.252   NaN  NaN           NaN         NaN\n",
       "8171       1953-03-01    0.000   NaN  NaN           NaN         NaN\n",
       "8172       1953-02-01  100.165   NaN  NaN           NaN         NaN\n",
       "8173       1953-01-01      NaN   NaN  NaN           NaN         NaN\n",
       "\n",
       "[8174 rows x 6 columns]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_macro = db.read_sql(query=\"SELECT * FROM macro\")\n",
    "df_macro_sorted = df_macro.pivot(\n",
    "    index=\"quote_date\",\n",
    "    columns=\"indicators\",\n",
    "    values=\"valor\"\n",
    ").sort_index(ascending=False).reset_index()\n",
    "df_macro_sorted[\"quote_date\"] = pd.to_datetime(df_macro_sorted[\"quote_date\"])\n",
    "df_macro_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = res.copy()\n",
    "df_diff[\"diff\"] = -(df_diff.reset_index()[\"quote_date\"].shift(1) - df_diff.index).dt.days.fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_macro = (\n",
    "    pd.DataFrame(df_diff.reset_index()[\"quote_date\"])\n",
    "    .merge(df_macro_sorted, \n",
    "           on=\"quote_date\", \n",
    "           how=\"left\")\n",
    "    .fillna(method=\"backfill\")\n",
    "    .fillna(0)\n",
    "    .set_index(\"quote_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAP = {company: index + 1 for index, company in enumerate(df.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the temporal part of Positional Encoding\n",
    "PE_T = 10\n",
    "\n",
    "# Price history\n",
    "HISTORY = 14\n",
    "\n",
    "# Number of days to test\n",
    "TEST_DAYS = 360\n",
    "\n",
    "# Forecast days\n",
    "HORIZON_FORECAST = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_component(df: pd.DataFrame, time: int, history: int, horizon: int) -> pd.DataFrame:\n",
    "    \"\"\"Based on the price database, \n",
    "    add the time component positional encoding \n",
    "\n",
    "    :param df: the dataframe in the window\n",
    "    :type df: pd.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the dataframe\n",
    "    df_range = df_diff[time-horizon:time+history]\n",
    "\n",
    "    # get prices\n",
    "    df_range_prices = df_range.xs(\"price\", axis=1, level=1,drop_level=True)\n",
    "\n",
    "    # Keep symbol where there is all predicted prices AND at least one history price\n",
    "    exist_price_history = (~df_range_prices.isna().all()) \n",
    "    exist_predicted_price = (~df_range_prices.isna().any())\n",
    "    keep = exist_price_history & exist_predicted_price\n",
    "    keep = pd.concat([keep,pd.Series({\"diff\": True})], axis=0)\n",
    "\n",
    "    df_range = df_range[keep.index[keep]]\n",
    "\n",
    "    # Companies\n",
    "    companies = list(set(df_range.columns.get_level_values(0)) - {\"diff\"})\n",
    "\n",
    "    # Define y\n",
    "    y_list = df_range_prices.iloc[:horizon][companies].round(5).T.values.tolist()\n",
    "        \n",
    "    # cumulated the time component\n",
    "    df_range.iloc[:horizon, -1] = 0\n",
    "    df_order = df_range.assign(order=lambda x: x['diff'].cumsum())\n",
    "    \n",
    "    return df_order, y_list, companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract prices history, futures and companies name\n",
    "df_prices, y, companies = add_time_component(df=df_diff,\n",
    "                    time=10,\n",
    "                    history=HISTORY,\n",
    "                    horizon=HORIZON_FORECAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5956/5956 [01:46<00:00, 55.77it/s] \n"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoding(PE_T)\n",
    "N = df.shape[0]\n",
    "\n",
    "data = {\n",
    "    \"train\": {},\n",
    "    \"test\": {},\n",
    "    \"pred\": {},\n",
    "    \"macro\": {}\n",
    "}\n",
    "\n",
    "for t in tqdm(range(HORIZON_FORECAST, N - HISTORY)):\n",
    "    \n",
    "    # extract prices history, futures and companies name\n",
    "    df_prices, y, companies = add_time_component(df=df_diff,\n",
    "                       time=t,\n",
    "                       history=HISTORY,\n",
    "                       horizon=HORIZON_FORECAST)\n",
    "    \n",
    "    \n",
    "    data[\"pred\"][t] = dict(zip(companies, y))\n",
    "    \n",
    "    macro_features = torch.tensor(df_macro.iloc[t].values, dtype=torch.float)\n",
    "    \n",
    "    if torch.isnan(macro_features).any():\n",
    "            print(\"NaN Detected In Macro\")\n",
    "    \n",
    "    data[\"macro\"][t] =macro_features\n",
    "    \n",
    "    d_t = {}\n",
    "    \n",
    "    for col in companies:\n",
    "        \n",
    "        # get company and order component\n",
    "        df_col = df_prices.loc[:, pd.IndexSlice[[col, 'order'], :]].dropna(subset = [(col, \"price\")]).fillna(0)\n",
    "        \n",
    "        # tensor\n",
    "        tensor_col = torch.tensor(df_col.values, dtype=torch.float)  \n",
    "        prices = tensor_col[HORIZON_FORECAST:,:-1]\n",
    "        pos = tensor_col[HORIZON_FORECAST:,-1].unsqueeze(-1)\n",
    "        \n",
    "        # encode the position\n",
    "        pos_enc = pe(pos)\n",
    "        \n",
    "        features = torch.concat((prices, pos_enc), dim = 1)\n",
    "        \n",
    "        if torch.isnan(features).any():\n",
    "            print(\"NaN Detected In Features\")\n",
    "\n",
    "        # add features in a list to pad later\n",
    "        d_t[col] = features\n",
    "        \n",
    "    if len(d_t) == 0:\n",
    "        continue\n",
    "        \n",
    "    if t < TEST_DAYS:\n",
    "        data[\"test\"][t] = d_t\n",
    "    else:\n",
    "        data[\"train\"][t] = d_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_SIZE = df_macro.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_loss(x, y):\n",
    "    # Ensure no division by zero\n",
    "    epsilon = 1e-6\n",
    "    loss = torch.abs(100 * (x - y) / (y + epsilon))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itub4 has 5 macro indicators\n",
      "bbdc4 has 5 macro indicators\n",
      "bbas3 has 5 macro indicators\n",
      "sanb11 has 5 macro indicators\n",
      "bpac11 has 5 macro indicators\n",
      "petr4 has 0 macro indicators\n"
     ]
    }
   ],
   "source": [
    "d_size = {}\n",
    "for comp in TARGETS_LOW:\n",
    "    size = df_diff.loc[:, (comp, slice(None))].shape[1]\n",
    "    d_size[comp] = size\n",
    "    print(f\"{comp} has {size-1} macro indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyExtractor(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CompanyExtractor, self).__init__()\n",
    "        self.lstm = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hn = self.lstm(x)\n",
    "        return hn[0]\n",
    "    \n",
    "lstm_models = ModuleDict({comp: CompanyExtractor(size + PE_T, OUT_LSTM_SIZE, HORIZON_FORECAST) for comp, size in d_size.items()})\n",
    "mlp_heads = ModuleDict({comp: torch.nn.Linear(OUT_GNN_SIZE + MACRO_SIZE, HORIZON_FORECAST) for comp in TARGETS_LOW})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "    ):\n",
    "        super(MyGNN, self).__init__()\n",
    "        self.gat_conv = GATConv(in_channels=in_channels, out_channels=out_channels)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        lstm_tensor = data\n",
    "        nbr_nodes = lstm_tensor.shape[0]\n",
    "        edge_index = torch.combinations(torch.arange(nbr_nodes)).t()\n",
    "        \n",
    "        x = self.gat_conv(lstm_tensor, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "my_gnn = MyGNN(in_channels = OUT_LSTM_SIZE,out_channels = OUT_GNN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_separatly(lstm_dict: torch.nn.ModuleDict, data_t: dict) -> torch.tensor:\n",
    "    \n",
    "    out_lstm = []\n",
    "    comps = []\n",
    "        \n",
    "    # Run each LSTM separatly and aggregate the result in the same matrix\n",
    "    for comp, tensor in data_t.items(): \n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        \n",
    "        out_comp_lstm = lstm_dict[comp](tensor)\n",
    "                        \n",
    "        # Getting the last output from LSTM\n",
    "        out_lstm.append(out_comp_lstm.squeeze(0))\n",
    "        comps.append(comp)\n",
    "                            \n",
    "    # Concatenate the outputs \n",
    "    features_extracted = torch.stack(out_lstm, dim=0).squeeze(1)  \n",
    "    \n",
    "    return features_extracted, comps\n",
    "\n",
    "def run_mlp_heads_separatly(mlp_heads: torch.nn.ModuleDict, features_encoded: torch.tensor, comps:list, pred_t: dict, macro: torch.tensor)-> torch.tensor:\n",
    "    \n",
    "    # Run each MLP separatly\n",
    "    price_outputs_time_t = []\n",
    "    pred_output_time_t = []\n",
    "    \n",
    "    for k, comp in enumerate(comps):\n",
    "        out_gnn_comp_i = features_encoded[k]\n",
    "        \n",
    "        gnn_with_macro = torch.concatenate([out_gnn_comp_i, macro])\n",
    "        \n",
    "        price_comp_i = mlp_heads[comp](gnn_with_macro)\n",
    "        price_outputs_time_t.append(price_comp_i)\n",
    "        pred_output_time_t.append(pred_t[comp])\n",
    "        \n",
    "    # Concatenate the outputs frm the LSTM\n",
    "    pred = torch.stack(price_outputs_time_t, dim=0)\n",
    "    \n",
    "    # Prepare ground truth from d_pred for the current timestep\n",
    "    true = torch.tensor(pred_output_time_t).reshape_as(pred).float()\n",
    "    \n",
    "    return pred, true\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_lstm_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310 - GAT.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m HYPERPARAM \u001b[39m=\u001b[39m {\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     out_lstm_size: \u001b[39m5\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     out_gnn_size: \u001b[39m5\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     criterion: \u001b[39m\"\u001b[39m\u001b[39mMSE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer: \u001b[39m\"\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     lr: \u001b[39m0.01\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     scheduler: \u001b[39m\"\u001b[39m\u001b[39mReduceLROnPlateau\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     patience: \u001b[39m8\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     factor: \u001b[39m0.8\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m RUN_CONFIG \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Dimension of the temporal part of Positional Encoding\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     PE_T: \u001b[39m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     HORIZON_FORECAST: \u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m TEST_CONFIG \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     use_gnn: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#Y220sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_lstm_size' is not defined"
     ]
    }
   ],
   "source": [
    "HYPERPARAM = {\n",
    "    out_lstm_size: 5,\n",
    "    out_gnn_size: 5,\n",
    "    criterion: \"MSE\",\n",
    "    optimizer: \"Adam\",\n",
    "    lr: 0.01,\n",
    "    scheduler: \"ReduceLROnPlateau\",\n",
    "    patience: 8,\n",
    "    factor: 0.8,\n",
    "    epochs = 300, \n",
    "}\n",
    "\n",
    "RUN_CONFIG = {\n",
    "    PE_T: 10,\n",
    "    HISTORY: 14, \n",
    "    TEST_DAYS: 360,\n",
    "    HORIZON_FORECAST: 1,\n",
    "}\n",
    "\n",
    "TEST_CONFIG = {\n",
    "    use_gnn: True,\n",
    "    var_macro: True,\n",
    "    var_fund: True,\n",
    "    stocks: \"Banks\" # or random \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 123.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 599.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Epoch [1/300], Train Loss: 45.4390, Test Loss: 42.2477'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 132.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 573.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Epoch [2/300], Train Loss: 16.6154, Test Loss: 34.3257'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 127.28it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 552.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Epoch [3/300], Train Loss: 13.6744, Test Loss: 34.8124'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 129.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 495.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Epoch [4/300], Train Loss: 12.7977, Test Loss: 31.9022'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 137.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 589.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Epoch [5/300], Train Loss: 13.4525, Test Loss: 33.0614'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 134.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 562.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Epoch [6/300], Train Loss: 13.1311, Test Loss: 37.8653'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 158/500 [00:01<00:02, 116.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310 - GAT.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#X30sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#X30sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#X30sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep() \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m avg_train_loss \u001b[39m=\u001b[39m total_train_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_timesteps)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valentincthrn/Desktop/TCC/StockMarketGNN/notebooks/20230310%20-%20GAT.ipynb#X30sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m my_gnn\u001b[39m.\u001b[39meval()  \u001b[39m# Set the model to evaluation mode\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TCC/StockMarketGNN/tcc-env/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TCC/StockMarketGNN/tcc-env/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Desktop/TCC/StockMarketGNN/tcc-env/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/TCC/StockMarketGNN/tcc-env/lib/python3.9/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/Desktop/TCC/StockMarketGNN/tcc-env/lib/python3.9/site-packages/torch/optim/adam.py:332\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    335\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "optimizer = Adam(list(lstm_models.parameters()) + list(my_gnn.parameters())+ list(mlp_heads.parameters()), lr=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.8, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "\n",
    "use_GNN = False\n",
    "\n",
    "results_loss = {\"train\": [], \"test\": []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    total_test_loss = 0.0\n",
    "    train_timesteps = list(data[\"train\"].keys())[:500]\n",
    "    test_timesteps = list(data[\"test\"].keys())[:10]\n",
    "    random.shuffle(train_timesteps)\n",
    "    \n",
    "    for timestep in tqdm(train_timesteps):\n",
    "        data_t = data[\"train\"][timestep]\n",
    "        pred_t = data[\"pred\"][timestep]\n",
    "        macro = data[\"macro\"][timestep]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # PHASE 1: LSTM EXTRACTION\n",
    "        features_extracted, comps = run_lstm_separatly(lstm_models, data_t)\n",
    "        \n",
    "        # PHASE 2: GNN EXTRACTION\n",
    "        if use_GNN:\n",
    "            features_encoded = my_gnn(features_extracted)\n",
    "        else:\n",
    "            features_encoded = features_extracted\n",
    "        \n",
    "        # PHASE 3: MLP HEAD EXTRACTION\n",
    "        pred, true = run_mlp_heads_separatly(mlp_heads, features_encoded, comps, pred_t, macro)\n",
    "            \n",
    "        # Compute the loss\n",
    "        loss = mape_loss(pred, true)\n",
    "        total_train_loss += loss.item()\n",
    "         \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "    avg_train_loss = total_train_loss/len(train_timesteps)\n",
    "    \n",
    "    my_gnn.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for timestep in tqdm(test_timesteps):\n",
    "            test_t = data[\"test\"][timestep]\n",
    "            pred_t = data[\"pred\"][timestep]\n",
    "            \n",
    "            # PHASE 1: LSTM EXTRACTION\n",
    "            features_extracted, comps = run_lstm_separatly(lstm_models, test_t)\n",
    "            \n",
    "            # PHASE 2: GNN EXTRACTION\n",
    "            if use_GNN:\n",
    "                features_encoded = my_gnn(features_extracted)\n",
    "            else:\n",
    "                features_encoded = features_extracted\n",
    "            \n",
    "            # PHASE 3: MLP HEAD EXTRACTION\n",
    "            pred, true = run_mlp_heads_separatly(mlp_heads, features_encoded, comps, pred_t, macro)\n",
    "            \n",
    "                \n",
    "            # Compute the loss\n",
    "            loss = mape_loss(pred, true)\n",
    "            total_test_loss += loss.item()\n",
    "            \n",
    "        avg_test_loss = total_test_loss/len(test_timesteps)\n",
    "    \n",
    "    results_loss[\"train\"].append(avg_train_loss)\n",
    "    results_loss[\"test\"].append(avg_test_loss)\n",
    "    # Update the learning rate\n",
    "    scheduler.step(avg_test_loss)\n",
    "    display(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "display(\"Training Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAIVE_COLS = [col + \"_naive\" for col in df.columns]\n",
    "df_naive = df.copy().iloc[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "df_naive = df.copy().iloc[:360]\n",
    "BASE_COLS = df_naive.columns\n",
    "for k in range(1, HORIZON_FORECAST+1):\n",
    "    NAIVE_COLS = [col + \"_naive_\" + str(k) for col in BASE_COLS]\n",
    "    # print(NAIVE_COLS)\n",
    "    # display(df_naive[BASE_COLS].shift(-k))\n",
    "    df_naive[NAIVE_COLS] = df_naive[BASE_COLS].shift(-k)\n",
    "    res_k = []\n",
    "    for col in df.columns:\n",
    "        x = df_naive[col]\n",
    "        y = df_naive[col + \"_naive_\" + str(k)]\n",
    "        res_k.append((abs(100*(x - y) / (y))).mean())\n",
    "    res.append(res_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3863443549317693\n",
      "2.019333779733331\n",
      "2.5116544197903066\n",
      "2.951522337770069\n",
      "3.3507373065127455\n"
     ]
    }
   ],
   "source": [
    "for e_k in res:\n",
    "    print(np.mean(e_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9520859410384888"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = \"close_b3sa3\"\n",
    "x = df_naive[col]\n",
    "y = df_naive[col + \"_naive\"]\n",
    "(abs(100*(x - y) / (y))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['close_b3sa3', 'close_bbdc4', 'close_bees3'], dtype='object')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.abs(100 * (x - y) / (y + epsilon))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
